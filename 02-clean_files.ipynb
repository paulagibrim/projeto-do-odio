{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 00 - Clean files",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:41:29.997486Z",
     "start_time": "2025-11-28T03:41:29.409217Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pandas pyarrow liac-arff fastparquet",
   "id": "fc7736b64da001e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (2.3.3)\r\n",
      "Requirement already satisfied: pyarrow in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (22.0.0)\r\n",
      "Requirement already satisfied: liac-arff in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (2.5.0)\r\n",
      "Requirement already satisfied: fastparquet in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (2024.11.0)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from pandas) (2.3.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: cramjam>=2.3 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from fastparquet) (2.11.0)\r\n",
      "Requirement already satisfied: fsspec in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from fastparquet) (2025.10.0)\r\n",
      "Requirement already satisfied: packaging in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from fastparquet) (25.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hurtlex",
   "id": "b6e462ace21c5b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:41:30.277884Z",
     "start_time": "2025-11-28T03:41:30.001183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Configurações iniciais\n",
    "lexic_file = './data/01-raw/HurtLex-PT/hurtlex_PT.tsv'\n",
    "\n",
    "# Criar o DataFrame Pandas\n",
    "raw_df = pd.read_csv(lexic_file, sep='\\t')\n",
    "\n",
    "# Separar os conservatives\n",
    "conservative_df = raw_df[raw_df['level'] == 'conservative']\n",
    "\n",
    "# Salvar como csv\n",
    "new_path = './data/02-cleaned/'\n",
    "os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "\n",
    "file_name = 'hurtlex_PT_conservatives.csv'\n",
    "conservative_df.to_csv(new_path + file_name, index=False)"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Datasets",
   "id": "28171bcb375d7316"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:41:30.380749Z",
     "start_time": "2025-11-28T03:41:30.281748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import arff  # Necessário para ler o arquivo .arff (OffComBR)\n",
    "import csv\n",
    "\n",
    "# Configurações iniciais\n",
    "datasets_dir = './data/01-raw/Datasets/'\n",
    "\n",
    "datasets = {\n",
    "\t\t'Fortuna'     :['Fortuna/2019-05-28_portuguese_hate_speech_binary_classification.csv'],\n",
    "\t\t'HateBRXplain':['HateBRXplain/dataset/HateBRXplain/HateBRXplain.csv'],\n",
    "\t\t'OffComBR'    :['OffComBR/OffComBR3.arff'],\n",
    "\t\t'OLID-BR'     :[\n",
    "\t\t\t\t'OLID-BR/test.csv',\n",
    "\t\t\t\t'OLID-BR/train.csv'\n",
    "\t\t\t\t],\n",
    "\t\t'ToLD-BR'     :['ToLD-BR/ToLD-BR.csv'],\n",
    "\t\t'TuPy'        :['TuPy/binary/tupy_binary_vote.csv']\n",
    "\t\t}\n",
    "\n",
    "# Dicionário para guardar o resultado final: {'NomeDataset': DataFrame}\n",
    "dfs_dict = {}\n",
    "\n",
    "for nome_dataset, lista_arquivos in datasets.items( ):\n",
    "\tlista_temp_dfs = []  # Lista temporária para casos com múltiplos arquivos (OLID-BR)\n",
    "\n",
    "\tfor arquivo in lista_arquivos:\n",
    "\t\t# Cria o caminho completo\n",
    "\t\tcaminho_completo = os.path.join(datasets_dir, arquivo)\n",
    "\n",
    "\t\t# Verifica a extensão e usa o leitor apropriado\n",
    "\t\tif arquivo.endswith('.csv'):\n",
    "\t\t\t# Atenção: alguns CSVs podem precisar de sep=';' ou encoding='latin1'\n",
    "\t\t\tdf = pd.read_csv(caminho_completo)\n",
    "\n",
    "\t\telif arquivo.endswith('.parquet'):\n",
    "\t\t\t# Adicionado engine='fastparquet' para evitar erro do PyArrow/Python 3.13\n",
    "\t\t\tdf = pd.read_parquet(caminho_completo, engine='fastparquet')\n",
    "\n",
    "\t\telif arquivo.endswith('.arff'):\n",
    "\t\t\twith open(caminho_completo, 'r') as f:\n",
    "\t\t\t\t# Carrega o ARFF como um dicionário\n",
    "\t\t\t\tdataset_arff = arff.load(f)\n",
    "\n",
    "\t\t\t\t# Extrai dados e nomes das colunas\n",
    "\t\t\t\tdados = dataset_arff['data']\n",
    "\t\t\t\tcolunas = [atributo[0] for atributo in dataset_arff['attributes']]\n",
    "\n",
    "\t\t\t\tdf = pd.DataFrame(dados, columns=colunas)\n",
    "\n",
    "\t\tlista_temp_dfs.append(df)\n",
    "\n",
    "\t# Concatena as partes (para o OLID-BR) e salva no dicionário principal\n",
    "\tif lista_temp_dfs:\n",
    "\t\tdfs_dict[nome_dataset] = pd.concat(lista_temp_dfs, ignore_index=True)\n",
    "\t\tprint(f\"Dataset '{nome_dataset}' carregado com sucesso. Shape: {dfs_dict[nome_dataset].shape}\")"
   ],
   "id": "51246fe679d2d65e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'Fortuna' carregado com sucesso. Shape: (5670, 8)\n",
      "Dataset 'HateBRXplain' carregado com sucesso. Shape: (7000, 6)\n",
      "Dataset 'OffComBR' carregado com sucesso. Shape: (1033, 2)\n",
      "Dataset 'OLID-BR' carregado com sucesso. Shape: (6952, 17)\n",
      "Dataset 'ToLD-BR' carregado com sucesso. Shape: (21000, 7)\n",
      "Dataset 'TuPy' carregado com sucesso. Shape: (10000, 2)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fortuna",
   "id": "9d86dd57c908c02a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:41:30.446492Z",
     "start_time": "2025-11-28T03:41:30.387237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('##### LIMPEZA DE DADOS: Fortuna (binary version)')\n",
    "\n",
    "# Pega o DataFrame\n",
    "fortuna_df = dfs_dict['Fortuna'].copy( )\n",
    "print(f\"# Número de registros (raw): {fortuna_df.shape[0]}\")\n",
    "\n",
    "# Pega as colunas desejadas\n",
    "columns_to_keep = ['text', 'hatespeech_comb']\n",
    "fortuna_df = fortuna_df[columns_to_keep]\n",
    "print(f\"#\\n# --- Colunas selecionadas: {columns_to_keep}\")\n",
    "\n",
    "# Padroniza os nomes das colunas\n",
    "fortuna_df.columns = ['text', 'is_toxic']\n",
    "print(f\"# --- Colunas renomeadas: {fortuna_df.columns}\")\n",
    "\n",
    "# Normalizar a coluna 'is_toxic'\n",
    "fortuna_df['is_toxic'] = fortuna_df['is_toxic'].map({'1':True, '0':False})\n",
    "print(f\"# --- Coluna 'is_toxic' normalizada para False (0) ou True (1)\")\n",
    "\n",
    "# Retira menções ao longo do texto, se antes do @ não for uma letra/palavra, e substitui por @user\n",
    "fortuna_df['text'] = fortuna_df['text'].str.replace(r'(?<!\\w)@\\w+', '@user', regex=True)\n",
    "fortuna_df['text'] = fortuna_df['text'].str.replace(r'\\bUSER\\b', '@user', regex=True)\n",
    "\n",
    "print(f\"# --- Menções normalizadas para '@user'\")\n",
    "\n",
    "# Retira URLs ao longo do texto\n",
    "fortuna_df['text'] = fortuna_df['text'].str.replace(r'(?:https?://|www\\.)\\S+', '<url>', regex=True)\n",
    "fortuna_df['text'] = fortuna_df['text'].str.replace(r'(?i)\\blink\\b(?=(?:\\s*link\\b)*\\s*$)', '<url>', regex=True)\n",
    "fortuna_df['text'] = fortuna_df['text'].str.replace(r'\\bURL\\b', '<url>', regex=True)\n",
    "print(f\"# --- Links normalizados para '<url>'\")\n",
    "\n",
    "# Normaliza os retweets, transformando para <RT>\n",
    "fortuna_df['text'] = fortuna_df['text'].str.replace(r'\\bRT\\b', '<RT>', regex=True)\n",
    "fortuna_df['text'] = fortuna_df['text'].str.replace(r'\\brt\\b', '<RT>', regex=True)\n",
    "print(f\"# --- Retweets normalizados para '<RT>'\")\n",
    "\n",
    "# Retira '\\n' e substitui por ' '\n",
    "fortuna_df['text'] = fortuna_df['text'].str.replace('\\n', ' ', regex=False)\n",
    "print(f\"# --- \\'\\\\n\\' normalizado para ' '\")\n",
    "\n",
    "# Desconsidera todos os is_toxic sem valor (None)\n",
    "fortuna_df = fortuna_df[fortuna_df['is_toxic'].notna( )]\n",
    "print(f\"#\\n# --- Registros com is_toxic=None removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de valores incompletos: {fortuna_df.shape[0]}\")\n",
    "\n",
    "# Desconsidera entradas duplicadas\n",
    "fortuna_df = fortuna_df.drop_duplicates(subset='text', keep='first')\n",
    "print(f\"#\\n# --- Registros duplicados removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de duplicatas: {fortuna_df.shape[0]}\")\n",
    "\n",
    "# Salva o DataFrame final como CSV\n",
    "fortuna_output = './data/02-cleaned/fortuna.csv'\n",
    "fortuna_df.to_csv(fortuna_output, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(f\"#\\n# Salvo em: {fortuna_output}\")"
   ],
   "id": "7705bd938fcc8888",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### LIMPEZA DE DADOS: Fortuna (binary version)\n",
      "# Número de registros (raw): 5670\n",
      "#\n",
      "# --- Colunas selecionadas: ['text', 'hatespeech_comb']\n",
      "# --- Colunas renomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para False (0) ou True (1)\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 0\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 0\n",
      "#\n",
      "# Salvo em: ./data/02-cleaned/fortuna.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### HateBRXplain",
   "id": "22d8cb7acc0598f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:41:30.514664Z",
     "start_time": "2025-11-28T03:41:30.451202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('##### LIMPEZA DE DADOS: HateBRXplain')\n",
    "\n",
    "# Pega o DataFrame\n",
    "hatebr_df = dfs_dict['HateBRXplain'].copy( )\n",
    "print(f\"# Número de registros (raw): {hatebr_df.shape[0]}\")\n",
    "\n",
    "# Pega as colunas desejadas\n",
    "columns_to_keep = ['comment', 'offensive_label']\n",
    "hatebr_df = hatebr_df[columns_to_keep]\n",
    "print(f\"#\\n# --- Colunas selecionadas: {columns_to_keep}\")\n",
    "\n",
    "# Padroniza os nomes das colunas\n",
    "hatebr_df.columns = ['text', 'is_toxic']\n",
    "print(f\"# --- Colunas renomeadas: {hatebr_df.columns}\")\n",
    "\n",
    "# Normalizar a coluna 'is_toxic'\n",
    "hatebr_df['is_toxic'] = hatebr_df['is_toxic'].map({'1':True, '0':False})\n",
    "print(f\"# --- Coluna 'is_toxic' normalizada para False (0) ou True (1)\")\n",
    "\n",
    "# Retira menções ao longo do texto, se antes do @ não for uma letra/palavra, e substitui por @user\n",
    "hatebr_df['text'] = hatebr_df['text'].str.replace(r'(?<!\\w)@\\w+', '@user', regex=True)\n",
    "hatebr_df['text'] = hatebr_df['text'].str.replace(r'\\bUSER\\b', '@user', regex=True)\n",
    "print(f\"# --- Menções normalizadas para '@user'\")\n",
    "\n",
    "# Retira URLs ao longo do texto\n",
    "hatebr_df['text'] = hatebr_df['text'].str.replace(r'(?:https?://|www\\.)\\S+', '<url>', regex=True)\n",
    "hatebr_df['text'] = hatebr_df['text'].str.replace(r'(?i)\\blink\\b(?=(?:\\s*link\\b)*\\s*$)', '<url>', regex=True)\n",
    "hatebr_df['text'] = hatebr_df['text'].str.replace(r'\\bURL\\b', '<url>', regex=True)\n",
    "print(f\"# --- Links normalizados para '<url>'\")\n",
    "\n",
    "# Normaliza os retweets, transformando para <RT>\n",
    "hatebr_df['text'] = hatebr_df['text'].str.replace(r'\\bRT\\b', '<RT>', regex=True)\n",
    "hatebr_df['text'] = hatebr_df['text'].str.replace(r'\\brt\\b', '<RT>', regex=True)\n",
    "print(f\"# --- Retweets normalizados para '<RT>'\")\n",
    "\n",
    "# Retira '\\n' e substitui por ' '\n",
    "hatebr_df['text'] = hatebr_df['text'].str.replace('\\n', ' ', regex=False)\n",
    "print(f\"# --- \\'\\\\n\\' normalizado para ' '\")\n",
    "\n",
    "# Desconsidera todos os is_toxic sem valor (None)\n",
    "hatebr_df = hatebr_df[hatebr_df['is_toxic'].notna( )]\n",
    "print(f\"#\\n# --- Registros com is_toxic=None removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de valores incompletos: {hatebr_df.shape[0]}\")\n",
    "\n",
    "# Desconsidera entradas duplicadas\n",
    "hatebr_df = hatebr_df.drop_duplicates(subset='text', keep='first')\n",
    "print(f\"#\\n# --- Registros duplicados removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de duplicatas: {hatebr_df.shape[0]}\")\n",
    "\n",
    "# Salva o DataFrame final como CSV\n",
    "hatebr_output = './data/02-cleaned/hatebrxplain.csv'\n",
    "hatebr_df.to_csv(hatebr_output, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(f\"#\\n# Salvo em: {hatebr_output}\")\n",
    "\n",
    "# hatebr_df.head(100)"
   ],
   "id": "14efdd7762df21ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### LIMPEZA DE DADOS: HateBRXplain\n",
      "# Número de registros (raw): 7000\n",
      "#\n",
      "# --- Colunas selecionadas: ['comment', 'offensive_label']\n",
      "# --- Colunas renomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para False (0) ou True (1)\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 0\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 0\n",
      "#\n",
      "# Salvo em: ./data/02-cleaned/hatebrxplain.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OffComBR3",
   "id": "450bc0fcb843fa89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:41:30.539995Z",
     "start_time": "2025-11-28T03:41:30.523686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('##### LIMPEZA DE DADOS: OffComBR3')\n",
    "\n",
    "# Pega o DataFrame\n",
    "offcombr_df = dfs_dict['OffComBR'].copy( )\n",
    "print(f\"# Número de registros (raw): {offcombr_df.shape[0]}\")\n",
    "\n",
    "# Pega as colunas desejadas\n",
    "columns_to_keep = ['document', '@@class']\n",
    "offcombr_df = offcombr_df[columns_to_keep]\n",
    "print(f\"#\\n# --- Colunas selecionadas: {columns_to_keep}\")\n",
    "\n",
    "# Padroniza os nomes das colunas\n",
    "offcombr_df.columns = ['text', 'is_toxic']\n",
    "print(f\"# --- Colunas renomeadas: {offcombr_df.columns}\")\n",
    "\n",
    "# Normalizar a coluna 'is_toxic'\n",
    "offcombr_df['is_toxic'] = offcombr_df['is_toxic'].map({'yes':True, 'no':False})\n",
    "print(f\"# --- Coluna 'is_toxic' normalizada para False (no) ou True (yes)\")\n",
    "\n",
    "# Retira menções ao longo do texto, se antes do @ não for uma letra/palavra, e substitui por @user\n",
    "offcombr_df['text'] = offcombr_df['text'].str.replace(r'(?<!\\w)@\\w+', '@user', regex=True)\n",
    "offcombr_df['text'] = offcombr_df['text'].str.replace(r'\\bUSER\\b', '@user', regex=True)\n",
    "\n",
    "print(f\"# --- Menções normalizadas para '@user'\")\n",
    "\n",
    "# Retira URLs ao longo do texto\n",
    "offcombr_df['text'] = offcombr_df['text'].str.replace(r'(?:https?://|www\\.)\\S+', '<url>', regex=True)\n",
    "offcombr_df['text'] = offcombr_df['text'].str.replace(r'(?i)\\blink\\b(?=(?:\\s*link\\b)*\\s*$)', '<url>', regex=True)\n",
    "offcombr_df['text'] = offcombr_df['text'].str.replace(r'\\bURL\\b', '<url>', regex=True)\n",
    "print(f\"# --- Links normalizados para '<url>'\")\n",
    "\n",
    "# Normaliza os retweets, transformando para <RT>\n",
    "offcombr_df['text'] = offcombr_df['text'].str.replace(r'\\bRT\\b', '<RT>', regex=True)\n",
    "offcombr_df['text'] = offcombr_df['text'].str.replace(r'\\brt\\b', '<RT>', regex=True)\n",
    "print(f\"# --- Retweets normalizados para '<RT>'\")\n",
    "\n",
    "# Retira '\\n' e substitui por ' '\n",
    "offcombr_df['text'] = offcombr_df['text'].str.replace('\\n', ' ', regex=False)\n",
    "print(f\"# --- \\'\\\\n\\' normalizado para ' '\")\n",
    "\n",
    "# Desconsidera todos os is_toxic sem valor (None)\n",
    "offcombr_df = offcombr_df[offcombr_df['is_toxic'].notna( )]\n",
    "print(f\"#\\n# --- Registros com is_toxic=None removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de valores incompletos: {offcombr_df.shape[0]}\")\n",
    "\n",
    "# Desconsidera entradas duplicadas\n",
    "offcombr_df = offcombr_df.drop_duplicates(subset='text', keep='first')\n",
    "print(f\"#\\n# --- Registros duplicados removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de duplicatas: {offcombr_df.shape[0]}\")\n",
    "\n",
    "# Salva o DataFrame final como CSV\n",
    "offcombr_output = './data/02-cleaned/offcombr3.csv'\n",
    "offcombr_df.to_csv(offcombr_output, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(f\"#\\n# Salvo em: {offcombr_output}\")\n",
    "\n",
    "# offcombr_df.head(100)"
   ],
   "id": "d697c6dc7c376c08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### LIMPEZA DE DADOS: OffComBR3\n",
      "# Número de registros (raw): 1033\n",
      "#\n",
      "# --- Colunas selecionadas: ['document', '@@class']\n",
      "# --- Colunas renomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para False (no) ou True (yes)\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 1033\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 1029\n",
      "#\n",
      "# Salvo em: ./data/02-cleaned/offcombr3.csv\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OLID",
   "id": "1279ad40d91fa197"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:41:30.646859Z",
     "start_time": "2025-11-28T03:41:30.547137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('##### LIMPEZA DE DADOS: OLID-BR (treino e teste)')\n",
    "\n",
    "# Pega o DataFrame\n",
    "olidbr_df = dfs_dict['OLID-BR'].copy( )\n",
    "print(f\"# Número de registros (raw): {olidbr_df.shape[0]}\")\n",
    "\n",
    "# Pega as colunas desejadas\n",
    "columns_to_keep = ['text', 'is_offensive']\n",
    "olidbr_df = olidbr_df[columns_to_keep]\n",
    "print(f\"#\\n# --- Colunas selecionadas: {columns_to_keep}\")\n",
    "\n",
    "# Padroniza os nomes das colunas\n",
    "olidbr_df.columns = ['text', 'is_toxic']\n",
    "print(f\"# --- Colunas renomeadas: {olidbr_df.columns}\")\n",
    "\n",
    "# Normalizar a coluna 'is_toxic'\n",
    "olidbr_df['is_toxic'] = olidbr_df['is_toxic'].map({'NOT':False, 'OFF':True})\n",
    "print(f\"# --- Coluna 'is_toxic' normalizada para False (NOT) ou True (OFF)\")\n",
    "\n",
    "# Retira menções ao longo do texto, se antes do @ não for uma letra/palavra, e substitui por @user\n",
    "# No caso desse dataset, já normalizado, faz o mesmo com 'USER', para padronizar com os outros datasets\n",
    "olidbr_df['text'] = olidbr_df['text'].str.replace(r'(?<!\\w)@\\w+', '@user', regex=True)\n",
    "olidbr_df['text'] = olidbr_df['text'].str.replace(r'\\bUSER\\b', '@user', regex=True)\n",
    "print(f\"# --- Menções normalizadas para '@user'\")\n",
    "\n",
    "# Retira URLs ao longo do texto\n",
    "# No caso desse dataset, já normalizado, faz o mesmo com 'URL', para padronizar com os outros datasets\n",
    "olidbr_df['text'] = olidbr_df['text'].str.replace(r'(?:https?://|www\\.)\\S+', '<url>', regex=True)\n",
    "olidbr_df['text'] = olidbr_df['text'].str.replace(r'\\bURL\\b', '<url>', regex=True)\n",
    "olidbr_df['text'] = olidbr_df['text'].str.replace(r'(?i)\\blink\\b(?=(?:\\s*link\\b)*\\s*$)', '<url>', regex=True)\n",
    "print(f\"# --- Links normalizados para '<url>'\")\n",
    "\n",
    "# Normaliza os retweets, transformando para <RT>\n",
    "olidbr_df['text'] = olidbr_df['text'].str.replace(r'\\bRT\\b', '<RT>', regex=True)\n",
    "olidbr_df['text'] = olidbr_df['text'].str.replace(r'\\brt\\b', '<RT>', regex=True)\n",
    "print(f\"# --- Retweets normalizados para '<RT>'\")\n",
    "\n",
    "# Retira '\\n' e substitui por ' '\n",
    "olidbr_df['text'] = olidbr_df['text'].str.replace('\\n', ' ', regex=False)\n",
    "print(f\"# --- \\'\\\\n\\' normalizado para ' '\")\n",
    "\n",
    "# Desconsidera todos os is_toxic sem valor (None)\n",
    "olidbr_df = olidbr_df[olidbr_df['is_toxic'].notna( )]\n",
    "print(f\"#\\n# --- Registros com is_toxic=None removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de valores incompletos: {olidbr_df.shape[0]}\")\n",
    "\n",
    "# Desconsidera entradas duplicadas\n",
    "olidbr_df = olidbr_df.drop_duplicates(subset='text', keep='first')\n",
    "print(f\"#\\n# --- Registros duplicados removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de duplicatas: {olidbr_df.shape[0]}\")\n",
    "\n",
    "# Salva o DataFrame final como CSV\n",
    "olidbr_output = './data/02-cleaned/olidbr.csv'\n",
    "olidbr_df.to_csv(olidbr_output, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(f\"#\\n# Salvo em: {olidbr_output}\")\n"
   ],
   "id": "5c77afd793f62e3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### LIMPEZA DE DADOS: OLID-BR (treino e teste)\n",
      "# Número de registros (raw): 6952\n",
      "#\n",
      "# --- Colunas selecionadas: ['text', 'is_offensive']\n",
      "# --- Colunas renomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para False (NOT) ou True (OFF)\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 6952\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 6952\n",
      "#\n",
      "# Salvo em: ./data/02-cleaned/olidbr.csv\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ToLD-BR",
   "id": "81f3ea78c077fcc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:41:30.839708Z",
     "start_time": "2025-11-28T03:41:30.651190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('##### LIMPEZA DE DADOS: ToLD-BR')\n",
    "\n",
    "# Pega o DataFrame\n",
    "toldbr_df = dfs_dict['ToLD-BR'].copy( )\n",
    "print(f\"# Número de registros (raw): {toldbr_df.shape[0]}\")\n",
    "\n",
    "# Pega as colunas desejadas\n",
    "# No caso desse dataset, precisamos identificar quais as mensagens que são toxicas -- quando tiver pelo menos um 1 nas colunas de tipo de ofensa/odio\n",
    "cols_toxicas = ['homophobia', 'obscene', 'insult', 'racism', 'misogyny', 'xenophobia']\n",
    "toldbr_df['is_toxic'] = toldbr_df[cols_toxicas].max(axis=1)\n",
    "print(f\"#\\n# --- Coluna criada: \\'is_toxic\\'\")\n",
    "\n",
    "toldbr_df = toldbr_df[['text', 'is_toxic']]\n",
    "\n",
    "# Padroniza os nomes das colunas\n",
    "# toldbr_df.columns = ['text', 'is_toxic']\n",
    "print(f\"# --- Colunas já nomeadas: {toldbr_df.columns}\")\n",
    "\n",
    "# Normalizar a coluna 'is_toxic'\n",
    "toldbr_df['is_toxic'] = toldbr_df['is_toxic'].map({'1.0':True, '0.0':False})\n",
    "print(f\"# --- Coluna 'is_toxic' normalizada para False (0) ou True (1)\")\n",
    "\n",
    "# Retira menções ao longo do texto, se antes do @ não for uma letra/palavra, e substitui por @user\n",
    "toldbr_df['text'] = toldbr_df['text'].str.replace(r'(?<!\\w)@\\w+', '@user', regex=True)\n",
    "toldbr_df['text'] = toldbr_df['text'].str.replace(r'\\bUSER\\b', '@user', regex=True)\n",
    "\n",
    "print(f\"# --- Menções normalizadas para '@user'\")\n",
    "\n",
    "# Retira URLs ao longo do texto\n",
    "# No caso esse foi anonimizado tb. Utilizaremos <url> quando link estiver por ultimo\n",
    "toldbr_df['text'] = toldbr_df['text'].str.replace(r'(?:https?://|www\\.)\\S+', '<url>', regex=True)\n",
    "toldbr_df['text'] = toldbr_df['text'].str.replace(r'(?i)\\blink\\b(?=(?:\\s*link\\b)*\\s*$)', '<url>', regex=True)\n",
    "toldbr_df['text'] = toldbr_df['text'].str.replace(r'\\bURL\\b', '<url>', regex=True)\n",
    "\n",
    "print(f\"# --- Links normalizados para '<url>'\")\n",
    "\n",
    "# Normaliza os retweets, transformando para <RT>\n",
    "toldbr_df['text'] = toldbr_df['text'].str.replace(r'\\bRT\\b', '<RT>', regex=True)\n",
    "toldbr_df['text'] = toldbr_df['text'].str.replace(r'\\brt\\b', '<RT>', regex=True)\n",
    "print(f\"# --- Retweets normalizados para '<RT>'\")\n",
    "\n",
    "\n",
    "# Retira '\\n' e substitui por ' '\n",
    "toldbr_df['text'] = toldbr_df['text'].str.replace('\\n', ' ', regex=False)\n",
    "print(f\"# --- \\'\\\\n\\' normalizado para ' '\")\n",
    "\n",
    "# Desconsidera todos os is_toxic sem valor (None)\n",
    "toldbr_df = toldbr_df[toldbr_df['is_toxic'].notna( )]\n",
    "print(f\"#\\n# --- Registros com is_toxic=None removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de valores incompletos: {toldbr_df.shape[0]}\")\n",
    "\n",
    "# Desconsidera entradas duplicadas\n",
    "toldbr_df = toldbr_df.drop_duplicates(subset='text', keep='first')\n",
    "print(f\"#\\n# --- Registros duplicados removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de duplicatas: {toldbr_df.shape[0]}\")\n",
    "\n",
    "# Salva o DataFrame final como CSV\n",
    "toldbr_output = './data/02-cleaned/toldbr.csv'\n",
    "toldbr_df.to_csv(toldbr_output, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(f\"#\\n# Salvo em: {toldbr_output}\")"
   ],
   "id": "a4278c184898da3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### LIMPEZA DE DADOS: ToLD-BR\n",
      "# Número de registros (raw): 21000\n",
      "#\n",
      "# --- Coluna criada: 'is_toxic'\n",
      "# --- Colunas já nomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para False (0) ou True (1)\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 0\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 0\n",
      "#\n",
      "# Salvo em: ./data/02-cleaned/toldbr.csv\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Typy",
   "id": "36318a92ffee65e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:41:30.924467Z",
     "start_time": "2025-11-28T03:41:30.843936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('##### LIMPEZA DE DADOS: TuPy (versão not-expanded)')\n",
    "\n",
    "# Pega o DataFrame\n",
    "tupy_df = dfs_dict['TuPy'].copy( )\n",
    "print(f\"# Número de registros (raw): {tupy_df.shape[0]}\")\n",
    "\n",
    "# Pega as colunas desejadas\n",
    "columns_to_keep = ['text', 'hate']\n",
    "tupy_df = tupy_df[columns_to_keep]\n",
    "print(f\"#\\n# --- Colunas selecionadas: {columns_to_keep}\")\n",
    "\n",
    "# Padroniza os nomes das colunas\n",
    "tupy_df.columns = ['text', 'is_toxic']\n",
    "print(f\"# --- Colunas renomeadas: {tupy_df.columns}\")\n",
    "\n",
    "# Normalizar a coluna 'is_toxic'\n",
    "tupy_df['is_toxic'] = tupy_df['is_toxic'].map({'1':True, '0':False})\n",
    "print(f\"# --- Coluna 'is_toxic' normalizada para False (0) ou True (1)\")\n",
    "\n",
    "# Retira menções ao longo do texto, se antes do @ não for uma letra/palavra, e substitui por @user\n",
    "tupy_df['text'] = tupy_df['text'].str.replace(r'(?<!\\w)@\\w+', '@user', regex=True)\n",
    "tupy_df['text'] = tupy_df['text'].str.replace(r'\\bUSER\\b', '@user', regex=True)\n",
    "print(f\"# --- Menções normalizadas para '@user'\")\n",
    "\n",
    "# Retira URLs ao longo do texto\n",
    "tupy_df['text'] = tupy_df['text'].str.replace(r'(?:https?://|www\\.)\\S+', '<url>', regex=True)\n",
    "tupy_df['text'] = tupy_df['text'].str.replace(r'(?i)\\blink\\b(?=(?:\\s*link\\b)*\\s*$)', '<url>', regex=True)\n",
    "tupy_df['text'] = tupy_df['text'].str.replace(r'\\bURL\\b', '<url>', regex=True)\n",
    "print(f\"# --- Links normalizados para '<url>'\")\n",
    "\n",
    "# Normaliza os retweets, transformando para <RT>\n",
    "tupy_df['text'] = tupy_df['text'].str.replace(r'\\bRT\\b', '<RT>', regex=True)\n",
    "tupy_df['text'] = tupy_df['text'].str.replace(r'\\brt\\b', '<RT>', regex=True)\n",
    "print(f\"# --- Retweets normalizados para '<RT>'\")\n",
    "\n",
    "\n",
    "# Retira '\\n' e substitui por ' '\n",
    "tupy_df['text'] = tupy_df['text'].str.replace('\\n', ' ', regex=False)\n",
    "print(f\"# --- \\'\\\\n\\' normalizado para ' '\")\n",
    "\n",
    "# Desconsidera todos os is_toxic sem valor (None)\n",
    "tupy_df = tupy_df[tupy_df['is_toxic'].notna( )]\n",
    "print(f\"#\\n# --- Registros com is_toxic=None removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de valores incompletos: {tupy_df.shape[0]}\")\n",
    "\n",
    "# Desconsidera entradas duplicadas\n",
    "tupy_df = tupy_df.drop_duplicates(subset='text', keep='first')\n",
    "print(f\"#\\n# --- Registros duplicados removidos\")\n",
    "print(f\"# Quantidade de registros após remoção de duplicatas: {tupy_df.shape[0]}\")\n",
    "\n",
    "# Salva o DataFrame final como CSV\n",
    "tupy_output = './data/02-cleaned/tupy.csv'\n",
    "tupy_df.to_csv(tupy_output, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(f\"#\\n# Salvo em: {tupy_output}\")\n"
   ],
   "id": "93d1f7b1ce2f23bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### LIMPEZA DE DADOS: TuPy (versão not-expanded)\n",
      "# Número de registros (raw): 10000\n",
      "#\n",
      "# --- Colunas selecionadas: ['text', 'hate']\n",
      "# --- Colunas renomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para False (0) ou True (1)\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 0\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 0\n",
      "#\n",
      "# Salvo em: ./data/02-cleaned/tupy.csv\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
