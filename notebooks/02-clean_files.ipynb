{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# 00 - Clean files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc7736b64da001e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:58:39.381200Z",
     "start_time": "2025-11-28T19:58:38.967406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (2.3.3)\r\n",
      "Requirement already satisfied: pyarrow in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (22.0.0)\r\n",
      "Requirement already satisfied: liac-arff in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (2.5.0)\r\n",
      "Requirement already satisfied: fastparquet in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (2024.11.0)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from pandas) (2.3.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: cramjam>=2.3 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from fastparquet) (2.11.0)\r\n",
      "Requirement already satisfied: fsspec in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from fastparquet) (2025.10.0)\r\n",
      "Requirement already satisfied: packaging in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from fastparquet) (25.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/gibrim/Documents/dev/JupyterProject/.odio/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyarrow liac-arff fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e462ace21c5b8",
   "metadata": {},
   "source": [
    "## Hurtlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44055338ad840d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:58:39.388011Z",
     "start_time": "2025-11-28T19:58:39.386265Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:58:39.400338Z",
     "start_time": "2025-11-28T19:58:39.392730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configurações iniciais\n",
    "lexic_file = '../data/01-raw/HurtLex-PT/hurtlex_PT.tsv'\n",
    "\n",
    "# Criar o DataFrame Pandas\n",
    "raw_df = pd.read_csv(lexic_file, sep='\\t')\n",
    "\n",
    "# Separar os conservatives\n",
    "conservative_df = raw_df[raw_df['level'] == 'conservative']\n",
    "\n",
    "# Salvar como csv\n",
    "new_path = '../data/02-cleaned/'\n",
    "os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "\n",
    "file_name = 'hurtlex_PT_conservatives.csv'\n",
    "conservative_df.to_csv(new_path + file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e072a46d7252a54",
   "metadata": {},
   "source": [
    "## Funções de Limpeza de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8762056aa74a2c6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:58:39.409192Z",
     "start_time": "2025-11-28T19:58:39.407833Z"
    }
   },
   "outputs": [],
   "source": [
    "import arff, csv\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a97a9aa46016133d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:59:19.840201Z",
     "start_time": "2025-11-28T19:59:19.836168Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cria um objeto dataset, para armazenar as informações necessárias\n",
    "class Dataset:\n",
    "\tdef __init__(self, name: str, orig_path: str | List[str]):\n",
    "\t\tself.name = name\n",
    "\t\tself.orig_path = orig_path\n",
    "\t\t# Normaliza para sempre ser uma lista, facilitando a iteração\n",
    "\t\tself.paths = orig_path if isinstance(orig_path, list) else [orig_path]\n",
    "\n",
    "\t\tself.output_path = f'../data/02-cleaned/{name.lower( )}.csv'\n",
    "\t\tself.df = self.get_df( )\n",
    "\n",
    "\t\tprint(f\"Dataset '{name}' criado com sucesso ({len(self.paths)} arquivos, com {self.df.shape[0]} registros)!\")\n",
    "\n",
    "\tdef get_df(self):\n",
    "\t\ttemp_dfs = []\n",
    "\n",
    "\t\tfor path in self.paths:\n",
    "\t\t\t# Verifica a extensão para escolher o leitor correto\n",
    "\t\t\tif path.endswith('.csv'):\n",
    "\t\t\t\t# Adicione sep= ou encoding= aqui se necessário para datasets específicos\n",
    "\t\t\t\tdf = pd.read_csv('../data/01-raw/Datasets/' + path)\n",
    "\n",
    "\t\t\telif path.endswith('.arff'):\n",
    "\t\t\t\twith open('../data/01-raw/Datasets/' + path, 'r') as f:\n",
    "\t\t\t\t\tobj = arff.load(f)\n",
    "\t\t\t\t\tdata = obj['data']\n",
    "\t\t\t\t\t# Extrai o nome dos atributos para usar como colunas\n",
    "\t\t\t\t\tattrs = [attr[0] for attr in obj['attributes']]\n",
    "\t\t\t\t\tdf = pd.DataFrame(data, columns=attrs)\n",
    "\n",
    "\t\t\telif path.endswith('.parquet'):\n",
    "\t\t\t\t# Caso precise ler parquet no futuro (ex: OLID original)\n",
    "\t\t\t\tdf = pd.read_parquet('../data/01-raw/Datasets/' + path, engine='fastparquet')\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(f\"Formato de arquivo não suportado: {path}\")\n",
    "\n",
    "\t\t\ttemp_dfs.append(df)\n",
    "\n",
    "\t\t# Se houver mais de um arquivo (ex: treino e teste), concatena um embaixo do outro\n",
    "\t\tif len(temp_dfs) > 1:\n",
    "\t\t\treturn pd.concat(temp_dfs, ignore_index=True)\n",
    "\n",
    "\t\treturn temp_dfs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1acdb9236c6e769",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:59:19.920787Z",
     "start_time": "2025-11-28T19:59:19.847878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'Fortuna' criado com sucesso (1 arquivos, com 5670 registros)!\n",
      "Dataset 'HateBRXplain' criado com sucesso (1 arquivos, com 7000 registros)!\n",
      "Dataset 'OffComBR3' criado com sucesso (1 arquivos, com 1033 registros)!\n",
      "Dataset 'OLID-BR' criado com sucesso (2 arquivos, com 6952 registros)!\n",
      "Dataset 'ToLD-BR' criado com sucesso (1 arquivos, com 21000 registros)!\n",
      "Dataset 'TuPy' criado com sucesso (1 arquivos, com 10000 registros)!\n"
     ]
    }
   ],
   "source": [
    "# Objetos para cada dataset\n",
    "fortuna_obj = Dataset('Fortuna', 'Fortuna/2019-05-28_portuguese_hate_speech_binary_classification.csv')\n",
    "hatebrxplain_obj = Dataset('HateBRXplain', 'HateBRXplain/dataset/HateBRXplain/HateBRXplain.csv')\n",
    "offcombr3_obj = Dataset('OffComBR3', 'OffComBR/OffComBR3.arff')\n",
    "olidbr_obj = Dataset('OLID-BR', ['OLID-BR/test.csv', 'OLID-BR/train.csv'])\n",
    "toldbr_obj = Dataset('ToLD-BR', 'ToLD-BR/ToLD-BR.csv')\n",
    "tupy_obj = Dataset('TuPy', 'TuPy/binary/tupy_binary_vote.csv')\n",
    "\n",
    "# Lista de todos os objetos\n",
    "datasets_obj = [fortuna_obj, hatebrxplain_obj, offcombr3_obj, olidbr_obj, toldbr_obj, tupy_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4505cafedc69df9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:59:19.928441Z",
     "start_time": "2025-11-28T19:59:19.924386Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_df_columns(df: pd.DataFrame, columns_to_keep: list, has_to_calc_toxic: bool, columns_to_calc: list, columns_name: list):\n",
    "\n",
    "\t# Se a coluna de toxicidade ainda precisa ser \"calculada\", cria uma nova coluna com 1 se alguma das columns_to_calc considerar toxico (for 1 ou mais), e 0 se nenhum.\n",
    "\tif has_to_calc_toxic:\n",
    "\t\tdf['is_toxic'] = df[columns_to_calc].max(axis=1)\n",
    "\t\tprint(f\"# --- Nova coluna criada: \\'is_toxic\\'\")\n",
    "\n",
    "\t\t# Adiciona a nova coluna às colunas desejadas\n",
    "\t\tcolumns_to_keep.append('is_toxic')\n",
    "\n",
    "\t# Faz o recorte do DataFrame para as colunas desejadas e as renomeia para o padrão.\n",
    "\tdf = df[columns_to_keep]\n",
    "\tprint(f\"# --- Colunas selecionadas: {df.columns}\")\n",
    "\tdf.columns = columns_name\n",
    "\tprint(f\"# --- Colunas renomeadas: {df.columns}\")\n",
    "\n",
    "\treturn df\n",
    "\n",
    "def get_normalized_toxic_col(df: pd.DataFrame, mapping: dict):\n",
    "\tprint(f\"# --- Coluna 'is_toxic' normalizada para booleano\")\n",
    "\treturn df['is_toxic'].map(mapping)\n",
    "\n",
    "def get_normalized_text_col(df: pd.DataFrame):\n",
    "\t# Retira menções ao longo do texto, se antes do @ não for uma letra/palavra, e substitui por @user\n",
    "\tdf['text'] = df['text'].str.replace(r'(?<!\\w)@\\w+', '@user', regex=True)\n",
    "\tdf['text'] = df['text'].str.replace(r'\\bUSER\\b', '@user', regex=True)\n",
    "\tprint(f\"# --- Menções normalizadas para '@user'\")\n",
    "\n",
    "\t# Retira URLs ao longo do texto\n",
    "\tdf['text'] = df['text'].str.replace(r'(?:https?://|www\\.)\\S+', '<url>', regex=True)\n",
    "\tdf['text'] = df['text'].str.replace(r'(?i)\\blink\\b(?=(?:\\s*link\\b)*\\s*$)', '<url>', regex=True)\n",
    "\tdf['text'] = df['text'].str.replace(r'\\bURL\\b', '<url>', regex=True)\n",
    "\tprint(f\"# --- Links normalizados para '<url>'\")\n",
    "\n",
    "\t# Normaliza os retweets, transformando para <RT>\n",
    "\tdf['text'] = df['text'].str.replace(r'\\bRT\\b', '<RT>', regex=True)\n",
    "\tdf['text'] = df['text'].str.replace(r'\\brt\\b', '<RT>', regex=True)\n",
    "\tprint(f\"# --- Retweets normalizados para '<RT>'\")\n",
    "\n",
    "\t# Retira '\\n' e substitui por ' '\n",
    "\tdf['text'] = df['text'].str.replace('\\n', ' ', regex=False)\n",
    "\tprint(f\"# --- \\'\\\\n\\' normalizado para ' '\")\n",
    "\n",
    "\treturn df['text']\n",
    "\n",
    "def remove_unexpressive_data(df: pd.DataFrame):\n",
    "\t# Desconsidera todos os is_toxic sem valor (None)\n",
    "\tdf = df[df['is_toxic'].notna( )]\n",
    "\tprint(f\"#\\n# --- Registros com is_toxic=None removidos\")\n",
    "\tprint(f\"# Quantidade de registros após remoção de valores incompletos: {df.shape[0]}\")\n",
    "\n",
    "\t# Desconsidera entradas duplicadas\n",
    "\tdf = df.drop_duplicates(subset='text', keep='first')\n",
    "\tprint(f\"#\\n# --- Registros duplicados removidos\")\n",
    "\tprint(f\"# Quantidade de registros após remoção de duplicatas: {df.shape[0]}\")\n",
    "\n",
    "\treturn df\n",
    "\n",
    "def save_final_df(dataset: Dataset):\n",
    "\t# Salva o DataFrame final como CSV\n",
    "\tdataset.df.to_csv(dataset.output_path, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\tprint(f\"#\\n# Salvo em: {dataset.output_path}\")\n",
    "\n",
    "# Função principal de limpeza de um dataset\n",
    "def clean_dataset(dataset: Dataset, columns_to_keep: list,\n",
    "\t\t\t\t  obs: str = None, has_to_calc_toxic: bool = False, columns_to_calc: list = None, columns_name: list=None,\n",
    "\t\t\t\t  toxic_mapping: dict=None\n",
    "\t\t\t\t  ):\n",
    "\n",
    "\tif toxic_mapping is None:\n",
    "\t\ttoxic_mapping = {'1':True, '0':False}\n",
    "\tif columns_name is None:\n",
    "\t\tcolumns_name = ['text', 'is_toxic']\n",
    "\n",
    "\tprint(f\"\\n########## LIMPEZA DE DADOS: {dataset.name}\")\n",
    "\n",
    "\t# Escreve alguma observação sobre o dataset, se desejado\n",
    "\tif obs is not None:\n",
    "\t\tprint(f\"# Observação: {obs}\\n#\\n\")\n",
    "\n",
    "\t# Faz uma cópia do df\n",
    "\tdf = dataset.df.copy( )\n",
    "\n",
    "\t# Escreve o número de registros do dataset\n",
    "\tprint(f\"# Número de registros (raw): {df.shape[0]}\")\n",
    "\n",
    "\t# Pega as colunas desejadas e padroniza os nomes\n",
    "\tdf = get_df_columns(df=df, columns_to_keep=columns_to_keep, has_to_calc_toxic=has_to_calc_toxic, columns_to_calc=columns_to_calc, columns_name=columns_name)\n",
    "\n",
    "\t# Normaliza a coluna 'is_toxic'\n",
    "\tdf['is_toxic'] = get_normalized_toxic_col(df=df, mapping=toxic_mapping)\n",
    "\n",
    "\t# Normaliza a coluna 'text'\n",
    "\tdf['text'] = get_normalized_text_col(df=df)\n",
    "\n",
    "\t# Remove dados duplicados ou sem anotação\n",
    "\tdf = remove_unexpressive_data(df=df)\n",
    "\n",
    "\t# Atualiza o df no dataset\n",
    "\tdataset.df = df\n",
    "\n",
    "\t# Salva o df final\n",
    "\tsave_final_df(dataset=dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "194e24a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:59:20.254863Z",
     "start_time": "2025-11-28T19:59:19.934795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## LIMPEZA DE DADOS: Fortuna\n",
      "# Observação: (binary version)\n",
      "#\n",
      "\n",
      "# Número de registros (raw): 5670\n",
      "# --- Colunas selecionadas: Index(['text', 'hatespeech_comb'], dtype='object')\n",
      "# --- Colunas renomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para booleano\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 5670\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 5666\n",
      "#\n",
      "# Salvo em: ../data/02-cleaned/fortuna.csv\n",
      "\n",
      "########## LIMPEZA DE DADOS: HateBRXplain\n",
      "# Número de registros (raw): 7000\n",
      "# --- Colunas selecionadas: Index(['comment', 'offensive_label'], dtype='object')\n",
      "# --- Colunas renomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para booleano\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 7000\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 7000\n",
      "#\n",
      "# Salvo em: ../data/02-cleaned/hatebrxplain.csv\n",
      "\n",
      "########## LIMPEZA DE DADOS: OffComBR3\n",
      "# Número de registros (raw): 1033\n",
      "# --- Colunas selecionadas: Index(['document', '@@class'], dtype='object')\n",
      "# --- Colunas renomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para booleano\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 1033\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 1029\n",
      "#\n",
      "# Salvo em: ../data/02-cleaned/offcombr3.csv\n",
      "\n",
      "########## LIMPEZA DE DADOS: OLID-BR\n",
      "# Observação: (treino e teste)\n",
      "#\n",
      "\n",
      "# Número de registros (raw): 6952\n",
      "# --- Colunas selecionadas: Index(['text', 'is_offensive'], dtype='object')\n",
      "# --- Colunas renomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para booleano\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 6952\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 6952\n",
      "#\n",
      "# Salvo em: ../data/02-cleaned/olid-br.csv\n",
      "\n",
      "########## LIMPEZA DE DADOS: ToLD-BR\n",
      "# Número de registros (raw): 21000\n",
      "# --- Nova coluna criada: 'is_toxic'\n",
      "# --- Colunas selecionadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Colunas renomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para booleano\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 16937\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 16806\n",
      "#\n",
      "# Salvo em: ../data/02-cleaned/told-br.csv\n",
      "\n",
      "########## LIMPEZA DE DADOS: TuPy\n",
      "# Observação: (versão not-expanded)\n",
      "#\n",
      "\n",
      "# Número de registros (raw): 10000\n",
      "# --- Colunas selecionadas: Index(['text', 'hate'], dtype='object')\n",
      "# --- Colunas renomeadas: Index(['text', 'is_toxic'], dtype='object')\n",
      "# --- Coluna 'is_toxic' normalizada para booleano\n",
      "# --- Menções normalizadas para '@user'\n",
      "# --- Links normalizados para '<url>'\n",
      "# --- Retweets normalizados para '<RT>'\n",
      "# --- '\\n' normalizado para ' '\n",
      "#\n",
      "# --- Registros com is_toxic=None removidos\n",
      "# Quantidade de registros após remoção de valores incompletos: 10000\n",
      "#\n",
      "# --- Registros duplicados removidos\n",
      "# Quantidade de registros após remoção de duplicatas: 9917\n",
      "#\n",
      "# Salvo em: ../data/02-cleaned/tupy.csv\n"
     ]
    }
   ],
   "source": [
    "clean_dataset(fortuna_obj, columns_to_keep = ['text', 'hatespeech_comb'], \n",
    "              obs='(binary version)',\n",
    "              toxic_mapping={1:True, 0:False})\n",
    "\n",
    "clean_dataset(hatebrxplain_obj, columns_to_keep = ['comment', 'offensive_label'],\n",
    "              toxic_mapping={1:True, 0:False})\n",
    "\n",
    "clean_dataset(offcombr3_obj, columns_to_keep = ['document', '@@class'],\n",
    "              toxic_mapping={'yes':True, 'no':False})\n",
    "\n",
    "clean_dataset(olidbr_obj, columns_to_keep = ['text', 'is_offensive'],\n",
    "              obs='(treino e teste)',\n",
    "              toxic_mapping={'NOT':False, 'OFF':True})\n",
    "\n",
    "clean_dataset(toldbr_obj, columns_to_keep=['text'], has_to_calc_toxic=True, \n",
    "              columns_to_calc=['homophobia', 'obscene', 'insult', 'racism', 'misogyny', 'xenophobia'], toxic_mapping={1:True, 0:False})\n",
    "\n",
    "clean_dataset(tupy_obj, columns_to_keep = ['text', 'hate'], obs='(versão not-expanded)',\n",
    "              toxic_mapping={1:True, 0:False})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
